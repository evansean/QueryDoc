# ğŸ§  QueryDoc â€” Multi-Source Conversational RAG System

QueryDoc is an interactive document assistant that lets you chat, summarize, and retrieve insights from multiple documents and websites â€” all in one conversational interface.

It demonstrates a full Retrieval-Augmented Generation (RAG) workflow:  
ğŸ“¥ multi-format ingestion â†’ ğŸ§± embedding & metadata tagging â†’ ğŸ” selective retrieval â†’ ğŸ’¬ conversational reasoning â€” powered by DeepSeek-V3, LangChain, and Chroma.

---

## ğŸš€ Features
- ğŸ“‚ **Multi-format ingestion** â€” Upload and process PDF, TXT, DOCX, CSV, and even URLs.  
- âš™ï¸ **Automatic parsing & embedding** â€” Each source is parsed, chunked, embedded, and stored in Chroma with descriptive metadata.  
- ğŸ·ï¸ **Metadata-driven categorization** â€” Every document chunk includes metadata like file name, type, and source URL.  
- ğŸ¯ **Metadata filtering** â€” Users can select specific documents or URLs to query; the retriever dynamically filters results based on those metadata fields.  
- ğŸ“š **Per-document summarization** â€” Each document is summarized individually using a map-reduce summarization chain for concise overviews.  
- ğŸ’¬ **Multi-source conversational chat** â€” Ask questions across multiple selected sources or all of them at once. The model fuses retrieved context from different documents to generate unified answers.  
- ğŸ§  **Memory-aware conversations** â€” Maintains multi-turn chat history with ConversationSummaryMemory, allowing context retention across user questions.  
- ğŸ” **Contextual retrieval (MMR)** â€” Uses Maximal Marginal Relevance to balance diversity and similarity for retrieved chunks.  
- ğŸ§© **Modular and extensible** â€” Clean function-based architecture for ingestion, summarization, and retrieval.  
- (Coming soon) âš–ï¸ **Cross-encoder reranking** â€” for even finer-grained result reordering after vector retrieval.

---

## ğŸ§© Architecture Overview
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚     Uploaded Files & URLs    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            Data Parsing Layer
  (PDF, DOCX, TXT, CSV, Webpage Extraction)
                     â”‚
                     â–¼
         Text Chunking & Embedding
      (LangChain + DeepSeek Embeddings)
                     â”‚
                     â–¼
     ğŸ§± Chroma Vector Database with Metadata
     (Stores embeddings + {source, type, name})
                     â”‚
                     â–¼
        Retriever (MMR + Metadata Filters)
     â†’ user-selected sources (multi-doc queries)
                     â”‚
                     â–¼
  RAG Chain + ConversationSummaryMemory
    (DeepSeek-V3 via Hugging Face Endpoint)
                     â”‚
                     â–¼
            Streamlit Interface
   (Summaries, Source Selection, and Chat)

---

## ğŸ§© How Each Component Meets the RAG Engineering Goals

| Requirement | Implementation in QueryDoc |
|-------------|----------------------------|
| **Data parsing from various formats** | Supports TXT, PDF, DOCX, CSV, and website URLs via Unstructured and LangChain loaders. |
| **Text embedding and vector DB integration** | Uses LangChain embeddings stored in Chroma, ensuring semantic search and fast retrieval. |
| **Categorization within vector DB** | Embeddings are tagged with metadata (source type, name, URL) for structured organization. |
| **Metadata filtering and multi-source querying** | The retriever applies metadata filters to limit results to selected files/URLs â€” enabling dynamic source selection at runtime. |
| **Efficient retrieval with memory management** | Combines MMR retriever with ConversationSummaryMemory to maintain both relevance and conversational flow. |
| **End-to-end data injection, parsing, and response generation** | From parsing â†’ embedding â†’ metadata tagging â†’ retrieval â†’ LLM answer generation, all in one seamless pipeline. |

---

## ğŸ’¡ Example Usage Flow
1. Upload files or enter URLs â€” load PDFs, Word docs, CSVs, or websites.  
2. The app parses and embeds each source into the Chroma vector database with metadata.  
3. Auto-generate summaries for each uploaded document.  
4. Select which sources to chat with using checkboxes or dropdowns.  
5. Ask questions naturally â€” the system retrieves relevant chunks only from selected sources and includes past chat context in responses.

---

## ğŸ§° Tech Stack

| Layer | Tools / Libraries |
|-------|------------------|
| Frontend | ğŸ–¥ï¸ Streamlit |
| LLM Backend | ğŸ¤– DeepSeek-V3 via Hugging Face API |
| Framework | ğŸ¦œ LangChain |
| Vector Store | ğŸ§± Chroma |
| Memory | ğŸ§  ConversationSummaryMemory |
| Environment | Python 3.10, dotenv, HuggingFace Hub |

---

## ğŸ§¾ Setup & Run

```bash
# Clone the repository
git clone https://github.com/<your-username>/QueryDoc.git
cd QueryDoc

# Install dependencies
pip install -r requirements.txt

# Set your Hugging Face token in Streamlit secrets or .env
HUGGINGFACEHUB_API_TOKEN=<your_token>

# Run the app
streamlit run app.py